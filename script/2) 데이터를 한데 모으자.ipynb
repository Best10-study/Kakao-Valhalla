{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "DATA_DIR = \"../data/\"\n",
    "RAW_DIR = os.path.join(DATA_DIR, \"raw\")\n",
    "PREP_DIR = os.path.join(DATA_DIR, \"prep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "    우리의 카카오 데이터를 한 데 모읍시다. 80기가가 되는데, 어떻게 줄여야 할지 고민해보아요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bcateid   의 메모리 size :    3MB(0.03%)\n",
      "brand     의 메모리 size :  124MB(1.38%)\n",
      "dcateid   의 메모리 size :    3MB(0.03%)\n",
      "img_feat  의 메모리 size : 7812MB(87.04%)\n",
      "maker     의 메모리 size :  274MB(3.05%)\n",
      "mcateid   의 메모리 size :    3MB(0.03%)\n",
      "model     의 메모리 size :  142MB(1.58%)\n",
      "pid       의 메모리 size :   11MB(0.12%)\n",
      "price     의 메모리 size :    3MB(0.03%)\n",
      "product   의 메모리 size :  583MB(6.50%)\n",
      "scateid   의 메모리 size :    3MB(0.03%)\n",
      "updttm    의 메모리 size :   14MB(0.16%)\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File(os.path.join(RAW_DIR,\"train.chunk.01\"),'r')\n",
    "\n",
    "# attribute 별 memory사이즈를 계산\n",
    "attr_size_dict = {}\n",
    "for key in f['train'].keys():\n",
    "    value = f['train'][key]\n",
    "    mb_size = (value.size * value.dtype.itemsize) // (1024**2)\n",
    "    attr_size_dict[key] = mb_size\n",
    "\n",
    "tot_size = sum(attr_size_dict.values())\n",
    "\n",
    "for key, value in attr_size_dict.items():\n",
    "    print(\"{:10}의 메모리 size : {:4}MB({:2.2f}%)\".format(\n",
    "        key,value,value/tot_size*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Feature가 차지하는 메모리 분량이 87%나 된다.\n",
    "즉 89기가 중 대략 77기가가 이미지 정보이고, 12기가 정도가 그 외 정보가 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 Feature를 제외하고 다른 Feature들은 하나의 h5 파일로 모아보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:42<00:00,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/prep/textOnly.h5에 저장되었습니다. (size : 11899mb)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "merge_dir = RAW_DIR\n",
    "save_path = os.path.join(PREP_DIR,\"textOnly.h5\") # 여기에 저장\n",
    "excludes = ['img_feat'] # img_feat 제거 \n",
    "\n",
    "# merge 하고자 하는 데이터 셋의 파일 path\n",
    "file_paths = [os.path.join(merge_dir, file_name)\n",
    "              for file_name in os.listdir(merge_dir)\n",
    "              if \"chunk\" in file_name]\n",
    "\n",
    "# Merge하고자 하는 attribute\n",
    "with h5py.File(file_paths[0], 'r') as f:\n",
    "    group_name = list(f.keys())[0]\n",
    "    attrs = list(f[group_name].keys())\n",
    "\n",
    "# 저장할 attribute 리스트\n",
    "attrs = list(set(attrs) - set(excludes))\n",
    "\n",
    "with h5py.File(save_path, 'w') as write_file:\n",
    "    for attr in tqdm(attrs):\n",
    "        values = {}\n",
    "        for file_path in file_paths:\n",
    "            # 데이터를 읽어들임\n",
    "            with h5py.File(file_path, 'r') as read_file:\n",
    "                group_name = list(read_file.keys())[0]\n",
    "                value = read_file[group_name][attr][:]\n",
    "\n",
    "            # subset 그룹별로 저장\n",
    "            if group_name not in values:\n",
    "                values[group_name] = [value]\n",
    "            else:\n",
    "                values[group_name].append(value)\n",
    "\n",
    "        # 나뉘어진 데이터들을 merge함\n",
    "        for subset_name in values.keys():\n",
    "            merge_value = np.concatenate(values[subset_name])\n",
    "\n",
    "            if subset_name not in write_file:\n",
    "                subset = write_file.create_group(subset_name)\n",
    "            else:\n",
    "                subset = write_file[subset_name]\n",
    "\n",
    "            subset.create_dataset(attr, data=merge_value)\n",
    "\n",
    "file_size = os.stat(save_path).st_size // (1024 ** 2)\n",
    "print(\"{}에 저장되었습니다. (size : {}mb)\".format(save_path, file_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단일 파일로 이제 11기가 정도의 파일로 줄어들었다. 로컬 컴퓨터에서도 이제 데이터 탐색이 가능한 사이즈로 줄어들었다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "직접 위의 코드를 돌리고 싶으면\n",
    "bash 창에서, \n",
    "\n",
    "```shell\n",
    "python valhalla/data.py merge ./data/raw/ ./data/prep/textOnly.h5\n",
    "```\n",
    "\n",
    "을 하면, 내부에 계층적으로 ['train','dev','test'] 식으로 group화되어 저장된다\n",
    "\n",
    "다음에서는 이 코드를 쉽고 효율적으로 처리하는 DataLoader를 구성할 것이다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
